% Learning rate
epsilon = 0.1;

% Softmax temperature
beta = 3;

% Feedback sensitivity
% Meant to scale the difference between received and current reward
% (estimate) (leave out for simplicity at first)
rho = 1;

numTrails = 1000;

%Initiate values for actions (2 valences times 2 actions)
Q = zeros(4,2);

env = Environment();

[state, correctAction] = env.presentTrial();

% Random for now, should be generated by model
actionCell = randsample({'Go', 'NoGo'}, 1);
action = actionCell{1};

reward = env.getReward(state, action);

conditionIdx = find(strcmp(env.conditions, state));
if strcmp(action, 'Go')
    actionIdx = 1;
else
    actionIdx = 2;
end
Q(conditionIdx, actionIdx) = Q(conditionIdx, actionIdx) + epsilon ...
    * (reward - Q(conditionIdx, actionIdx));
